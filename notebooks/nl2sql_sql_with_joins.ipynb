{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating SQL with JOINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/home/jupyter/git_repo/nl2sql-generic/nl2sql_src', '../', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(1, '../')\n",
    "sys.path.insert(1, '/home/jupyter/git_repo/nl2sql-generic/nl2sql_src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from vertexai.preview.generative_models import GenerativeModel, GenerationResponse, Tool\n",
    "from nl2sql_generic import Nl2sqlBq\n",
    "\n",
    "import json\n",
    "from prompts import * \n",
    "\n",
    "from proto.marshal.collections import repeated\n",
    "from proto.marshal.collections import maps\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from vertexai.language_models import CodeChatSession\n",
    "from vertexai.language_models import CodeChatModel\n",
    "\n",
    "from vertexai.language_models import CodeGenerationModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'sql-test-project-353312'\n",
    "DATASET_ID = 'zoominfo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing when metadata cache is already created\n",
    "metadata_cache_file = \"../nl2sql_src/cache_metadata/metadata_cache.json\"\n",
    "metadata_json_path = \"../nl2sql_src/cache_metadata/metadata_cache.json\"\n",
    "\n",
    "nl2sqlbq_client = Nl2sqlBq(project_id=PROJECT_ID, dataset_id=DATASET_ID, metadata_json_path = metadata_cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Initiated\n"
     ]
    }
   ],
   "source": [
    "PGPROJ = \"sl-test-project-353312\"\n",
    "PGLOCATION = 'us-central1'\n",
    "PGINSTANCE = \"test-nl2sql\"\n",
    "PGDB = \"test-db\"\n",
    "PGUSER = \"postgres\"\n",
    "PGPWD = \"test-nl2sql\"\n",
    "nl2sqlbq_client.init_pgdb(PGPROJ, PGLOCATION, PGINSTANCE, PGDB, PGUSER, PGPWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vertexai.preview.generative_models import GenerativeModel\n",
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "\n",
    "table_chat = model.start_chat()\n",
    "sql_chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_table_details(table_name):\n",
    "    f = open(metadata_json_path, encoding=\"utf-8\")\n",
    "    metadata_json = json.loads(f.read())\n",
    "        \n",
    "    table_json = metadata_json[table_name]\n",
    "    columns_json = table_json[\"Columns\"]\n",
    "    columns_info = \"\"\n",
    "    for column_name in columns_json:\n",
    "        column = columns_json[column_name]            \n",
    "        column_info = f\"\"\"{column[\"Name\"]} \\\n",
    "                    ({column[\"Type\"]}) : {column[\"Description\"]}. {column[\"Examples\"]}\\n\"\"\"\n",
    "        columns_info = columns_info + column_info\n",
    "        \n",
    "    prompt = Table_info_template.format(table_name = table_name,\n",
    "                                        table_description = metadata_json[table_name]['Description'],\n",
    "                                        columns_info = columns_info)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name_1 = \"calhhs-dashboard-2015-2020-annual-data-file\"\n",
    "table_name_2 = \"medi-cal-and-calfresh-enrollment\"\n",
    "table_1 = return_table_details(table_name_1)\n",
    "table_2 = return_table_details(table_name_2)\n",
    "sample_question = \"Which five counties have the lowest number of CalFresh authorized vendors compared to CalFresh participants?\"\n",
    "sample_sql = \"\"\"SELECT Vendor_Location,(vendor_cnt/total_participants)*100 as vendor_participants_ratio FROM\n",
    "((SELECT TRIM(Vendor_Location) AS Vendor_Location,COALESCE(SUM(SAFE_CAST(_Number_of_Participants_Redeemed_ AS INT64))) as total_participants FROM `cdii-poc.HHS_Program_Counts.calfresh-redemption-by-county-by-participant-category-data-2010-2018`  group by Vendor_Location) as participants\n",
    "JOIN\n",
    "(SELECT TRIM(COUNTY) AS COUNTY,count(VENDOR) as vendor_cnt FROM `cdii-poc.HHS_Program_Counts.women-infants-and-children-wic-authorized-vendors` \n",
    "group by COUNTY having COUNTY is not null) as vendors\n",
    "ON UPPER(participants.Vendor_Location)=UPPER(vendors.COUNTY))\n",
    "WHERE (vendor_cnt/total_participants)*100 is not null\n",
    "order by vendor_participants_ratio asc limit 5;\"\"\"\n",
    "\n",
    "# question = \"Which counties have the highest and lowest ratios of providers to enrolled participants in Medi-cal?\"\n",
    "question = \"Which five counties have the lowest number of WIC authorized vendors compared to WIC participants?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_prompt = join_prompt_template.format(table_1 = table_1,\n",
    "                                          table_2 = table_2,\n",
    "                                          question = question)\n",
    "join_prompt_one_shot = join_prompt_template_one_shot.format(table_1 = table_1,\n",
    "                                          table_2 = table_2,\n",
    "                                          sample_question = sample_question,\n",
    "                                          sample_sql = sample_sql,\n",
    "                                          question = question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Generation using NL2SQL Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names=[table_name_1, table_name_2]\n",
    "samples={\n",
    "    \"sample_question\":sample_question,\n",
    "    \"sample_sql\":sample_sql}\n",
    "\n",
    "table_names[1]\n",
    "print(\"*\" * 30)\n",
    "print(\"Zero-shot\")\n",
    "gen_sql = nl2sqlbq_client.invoke_llm(join_prompt)\n",
    "print(gen_sql)\n",
    "\n",
    "print(\"*\" * 30)\n",
    "print(\"One-shot\")\n",
    "gen_sql_os = nl2sqlbq_client.invoke_llm(join_prompt_one_shot)\n",
    "print(gen_sql_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating SQL using Gemini Pro for same prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot prompting :\n",
      " ```sql\n",
      "SELECT\n",
      "  c.county,\n",
      "  a.number_of_beneficiaries,\n",
      "  (SELECT COUNT(*) FROM WIC_Vendors) AS TotalWICVendors\n",
      "FROM calhhs_dashboard_2015_2020_annual_data_file AS a\n",
      "JOIN medi_cal_and_calfresh_enrollment AS c\n",
      "  ON (\n",
      "    a.program = c.program || \" only\"\n",
      "  )\n",
      "WHERE\n",
      "  a.program = \"WIC\"\n",
      "  AND c.program = \"CalFresh only\"\n",
      "ORDER BY\n",
      "  a.number_of_beneficiaries\n",
      "LIMIT 5;\n",
      "```\n",
      "One-shot prompt: \n",
      " ```sql\n",
      "SELECT\n",
      "  Vendor_Location,\n",
      "  (\n",
      "    vendor_cnt / total_participants\n",
      "  ) * 100 AS vendor_participants_ratio\n",
      "FROM (\n",
      "  (\n",
      "    SELECT\n",
      "      TRIM(Vendor_Location) AS Vendor_Location,\n",
      "      COALESCE(SUM(SAFE_CAST(_Number_of_Participants_Redeemed_ AS INT64))) AS total_participants\n",
      "    FROM `cdii-poc.HHS_Program_Counts.calfresh-redemption-by-county-by-participant-category-data-2010-2018`\n",
      "    GROUP BY\n",
      "      Vendor_Location\n",
      "  ) AS participants\n",
      "  JOIN (\n",
      "    SELECT\n",
      "      TRIM(COUNTY) AS COUNTY,\n",
      "      COUNT(VENDOR) AS vendor_cnt\n",
      "    FROM `cdii-poc.HHS_Program_Counts.women-infants-and-children-wic-authorized-vendors`\n",
      "    GROUP BY\n",
      "      COUNTY\n",
      "    HAVING\n",
      "      COUNTY IS NOT NULL\n",
      "  ) AS vendors\n",
      "    ON UPPER(participants.Vendor_Location) = UPPER(vendors.COUNTY)\n",
      ")\n",
      "WHERE\n",
      "  (vendor_cnt / total_participants) * 100 IS NOT NULL\n",
      "ORDER BY\n",
      "  vendor_participants_ratio ASC\n",
      "LIMIT 5;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "resp = model.generate_content(join_prompt)\n",
    "print(\"Zero shot prompting :\\n\", resp.text)\n",
    "\n",
    "resp = model.generate_content(join_prompt_one_shot)\n",
    "print(\"One-shot prompt: \\n\", resp.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = nl2sqlbq_client.table_filter(question)\n",
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_table_prompt = \"\"\"\n",
    "# Tables context:\n",
    "# {table_info}\n",
    "\n",
    "# Example Question, SQL and tables containing the required info are given below\n",
    "# You are required to identify more than 1 table that probably contains the information requested in the question given below\n",
    "# Return the list of tables that may contain the information\n",
    "\n",
    "# Question : {example_question} :\n",
    "# SQL : {example_SQL}\n",
    "# Tables: {table_name_1} and {table_name_2}\n",
    "\n",
    "# Question: {question}\n",
    "# Tables:\n",
    "# \"\"\"\n",
    "\n",
    "tab_prompt = nl2sqlbq_client.table_filter_promptonly(question)\n",
    "# print(tab_prompt)\n",
    "\n",
    "multi_prompt=multi_table_prompt.format(table_info=tab_prompt, \n",
    "                                       example_question=sample_question,\n",
    "                                       example_SQL=sample_sql,\n",
    "                                       table_name_1=table_name_1,\n",
    "                                       table_name_2=table_name_2,\n",
    "                                       question=question)\n",
    "print(multi_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chat = model.start_chat()\n",
    "responses = multi_chat.send_message(multi_prompt) #, tools=[sql_tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow_up_prompt = \"\"\"Review the question given in above context along with the table and column description and determine whether one table contains all the required information or you need to get data from another table\n",
    "# If two tables's information are required, then identify those tables from the tables info\n",
    "# What are the two tables that should be joined in the SQL query\n",
    "# Only mention the table name from the tables context.\n",
    "# \"\"\"\n",
    "resp = multi_chat.send_message(follow_up_prompt)\n",
    "res = resp.candidates[0].content.parts[0].text.split(' and ')\n",
    "\n",
    "table_1 = return_table_details(res[2].split(' ')[1].strip())\n",
    "table_2 = return_table_details(res[3].split(' ')[1].strip())\n",
    "\n",
    "join_prompt = join_prompt_template.format(table_1 = table_1,\n",
    "                                          table_2 = table_2,\n",
    "                                          question = question)\n",
    "join_prompt_one_shot = join_prompt_template_one_shot.format(table_1 = table_1,\n",
    "                                          table_2 = table_2,\n",
    "                                          sample_question = sample_question,\n",
    "                                          sample_sql = sample_sql,\n",
    "                                          question = question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model.generate_content(join_prompt)\n",
    "print(\"Zero shot prompting :\\n\", resp.text)\n",
    "\n",
    "resp = model.generate_content(join_prompt_one_shot)\n",
    "print(\"One-shot prompt: \\n\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Correction Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.3\n",
    "MAX_OUTPUT_TOKENS=8192\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "CODE_MODEL = \"code-bison-32k\"\n",
    "MODEL_NAME = 'codechat-bison-32k'\n",
    "\n",
    "# code_gen_model = CodeGenerationModel.from_pretrained(CODE_MODEL)\n",
    "code_gen_model = CodeChatModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_execute_sql(prompt, max_tries=5, return_all=False):\n",
    "    \"\"\"\n",
    "    Generate an SQL query using the code_gen_model, execute it using bq_client, and rank successful queries by latency.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): Prompt to provide to the model for generating SQL.\n",
    "    - max_tries (int): Maximum number of attempts to generate and execute SQL.\n",
    "    - return_all (bool): Flag to determine whether to return all successful queries or only the fastest.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the fastest dataframe or all successful dataframes, or error messages and prompt evolution.\n",
    "    \"\"\"\n",
    "    \n",
    "    tries = 0\n",
    "    error_messages = []\n",
    "    prompts = [prompt]\n",
    "    successful_queries = []\n",
    "\n",
    "    # chat_session = CodeChatSession(model=model, \n",
    "    #                                temperature=TEMPERATURE, \n",
    "    #                                max_output_tokens=MAX_OUTPUT_TOKENS)\n",
    "    \n",
    "    chat_session = model.start_chat()\n",
    "    \n",
    "#     chat_session = CodeChatSession(model=code_gen_model, \n",
    "#                                    temperature=TEMPERATURE, \n",
    "#                                    max_output_tokens=MAX_OUTPUT_TOKENS)\n",
    "    \n",
    "    while tries < max_tries:\n",
    "        logger.info(f'TRIAL: {tries+1}')\n",
    "        try:\n",
    "            # Predict SQL using the model\n",
    "            start_time = time.time()\n",
    "            # print(\"prommpt = \", prompt)\n",
    "            \n",
    "            response = chat_session.send_message(prompt)#, temperature=TEMPERATURE, max_output_tokens=MAX_OUTPUT_TOKENS)\n",
    "            # response = code_gen_model.predict(prompt, temperature=TEMPERATURE, max_output_tokens=MAX_OUTPUT_TOKENS)\n",
    "            \n",
    "            generated_sql_query = response.text\n",
    "            generated_sql_query = '\\n'.join(generated_sql_query.split('\\n')[1:-1])\n",
    "\n",
    "            generated_sql_query = nl2sqlbq_client.case_handler_transform(generated_sql_query)\n",
    "            generated_sql_query = nl2sqlbq_client.add_dataset_to_query(generated_sql_query)\n",
    "            \n",
    "            logger.info('-' * 50)\n",
    "            logger.info(generated_sql_query)\n",
    "            logger.info('-' * 50)\n",
    "            # Execute SQL using BigQuery client\n",
    "            df = bq_client.query(generated_sql_query).to_dataframe()\n",
    "            print(\"Data - \", df)\n",
    "            latency = time.time() - start_time\n",
    "            successful_queries.append({\n",
    "                \"query\": generated_sql_query,\n",
    "                \"dataframe\": df,\n",
    "                \"latency\": latency\n",
    "            })\n",
    "            logger.info('SUCCEEDED')\n",
    "            # Evolve the prompt for success path to optimize the last successful query for latency\n",
    "            if len(successful_queries) > 1:\n",
    "                prompt = f\"\"\"Modify the last successful SQL query by making changes to it and optimizing it for latency. \n",
    "            ENSURE that the NEW QUERY is DIFFERENT from the previous one while prioritizing faster execution.\n",
    "            Reference the tables only from the above given project and dataset\n",
    "            The last successful query was:\n",
    "            {successful_queries[-1][\"query\"]}\"\"\"\n",
    "        except Exception as e:\n",
    "            logger.error('FAILED')\n",
    "            # Catch the error, store the message, and try again\n",
    "            msg = str(e)\n",
    "            error_messages.append(msg)\n",
    "            if tries == 0:\n",
    "                generated_sql_query = gen_sql_os\n",
    "                \n",
    "            # Evolve the prompt by appending the error message and asking the model to correct it\n",
    "            prompt = f\"\"\"Encountered an error: {msg}. \n",
    "To address this, please generate an alternative SQL query response that avoids this specific error. \n",
    "Follow the instructions mentioned above to remediate the error. \n",
    "\n",
    "Modify the below SQL query to resolve the issue and ensure it is not a repetition of all previously generated queries.\n",
    "{generated_sql_query}\n",
    "            \n",
    "Ensure the revised SQL query aligns precisely with the requirements outlined in the initial question.\n",
    "Keep the table names as it is. Do not change hyphen to underscore character\n",
    "Additionally, please optimize the query for latency while maintaining correctness and efficiency.\"\"\"\n",
    "            prompts.append(prompt)\n",
    "        logger.info('=' * 100)\n",
    "        tries += 1\n",
    "        \n",
    "    # If no successful queries\n",
    "    if len(successful_queries) == 0:\n",
    "        return {\n",
    "            \"error\": \"All attempts exhausted.\",\n",
    "            \"prompts\": prompts,\n",
    "            \"errors\": error_messages\n",
    "        }\n",
    "    \n",
    "    # Sort successful queries by latency\n",
    "    successful_queries.sort(key=lambda x: x['latency'])\n",
    "    \n",
    "    if return_all:\n",
    "        df = pd.DataFrame([(q[\"query\"], q[\"dataframe\"], q[\"latency\"]) for q in successful_queries], columns=[\"Query\", \"Result\", \"Latency\"])\n",
    "        return {\n",
    "            \"dataframe\": df\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"fastest_query\": successful_queries[0][\"query\"],\n",
    "            \"result\": successful_queries[0][\"dataframe\"],\n",
    "            \"latency\": successful_queries[0][\"latency\"]\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_and_execute_sql(prompt=join_prompt_one_shot, return_all=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu113:m118"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
