{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn SQL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from vertexai.preview.generative_models import GenerativeModel, GenerationResponse, Tool\n",
    "from nl2sql_generic import Nl2sqlBq\n",
    "\n",
    "from proto.marshal.collections import repeated\n",
    "from proto.marshal.collections import maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing when metadata cache is already created\n",
    "metadata_cache_file = \"../nl2sql_src/cache_metadata/metadata_cache.json\"\n",
    "nl2sqlbq_client = Nl2sqlBq(project_id=\"sl-test-project-353312\", dataset_id=\"EY\", metadata_json_path = metadata_cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGPROJ = \"sl-test-project-353312\"\n",
    "PGLOCATION = 'us-central1'\n",
    "PGINSTANCE = \"test-nl2sql\"\n",
    "PGDB = \"test-db\"\n",
    "PGUSER = \"postgres\"\n",
    "PGPWD = \"test-nl2sql\"\n",
    "nl2sqlbq_client.init_pgdb(PGPROJ, PGLOCATION, PGINSTANCE, PGDB, PGUSER, PGPWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(resp: GenerationResponse):\n",
    "    part = resp.candidates[0].content.parts[0]\n",
    "    try:\n",
    "        text = part.text\n",
    "    except:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "def prior_sql_result(question)->str:\n",
    "    # Execute the SQL and return the result\n",
    "    return \"Test output for question : \" + question\n",
    "\n",
    "\n",
    "def call_api(name: str, args: str) -> str:\n",
    "    if name == \"prior_sql_tool\":\n",
    "        return prior_sql_result(args)\n",
    "\n",
    "\n",
    "def recurse_proto_repeated_composite(repeated_object):\n",
    "    repeated_list = []\n",
    "    for item in repeated_object:\n",
    "        if isinstance(item, repeated.RepeatedComposite):\n",
    "            item = recurse_proto_repeated_composite(item)\n",
    "            repeated_list.append(item)\n",
    "        elif isinstance(item, maps.MapComposite):\n",
    "            item = recurse_proto_marshal_to_dict(item)\n",
    "            repeated_list.append(item)\n",
    "        else:\n",
    "            repeated_list.append(item)\n",
    "\n",
    "    return repeated_list\n",
    "\n",
    "def recurse_proto_marshal_to_dict(marshal_object):\n",
    "    new_dict = {}\n",
    "    for k, v in marshal_object.items():\n",
    "      if not v:\n",
    "        continue\n",
    "      elif isinstance(v, maps.MapComposite):\n",
    "          v = recurse_proto_marshal_to_dict(v)\n",
    "      elif isinstance(v, repeated.RepeatedComposite):\n",
    "          v = recurse_proto_repeated_composite(v)\n",
    "      new_dict[k] = v\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "previous_sql_spec = {\n",
    "    \"name\": \"prior_sql_tool\",\n",
    "    \"description\": \"Provides the SQL query that is generated for the previous question\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The natural language question for which the SQL query was generated\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"question\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_function_name(response: GenerationResponse):\n",
    "  return response.candidates[0].content.parts[0].function_call.name\n",
    "\n",
    "def get_function_args(response: GenerationResponse) -> dict:\n",
    "  return recurse_proto_marshal_to_dict(response.candidates[0].content.parts[0].function_call.args)\n",
    "\n",
    "\n",
    "sql_tools = Tool.from_dict(\n",
    "    {\n",
    "        \"function_declarations\":[previous_sql_spec]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vertexai.preview.generative_models import GenerativeModel\n",
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "\n",
    "table_chat = model.start_chat()\n",
    "sql_chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_response(chat_model, prompt) -> str:\n",
    "    responses = chat_model.send_message(prompt, stream=True)\n",
    "    output = []\n",
    "    for response in responses:\n",
    "        output.append(response.candidates[0].content.parts[0].text)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"How many people are enrolled in CalFresh?\",\n",
    "             \"How many of them live in Los Angeles County?\"\n",
    "            ]\n",
    "\n",
    "# questions = [\"How many Black individuals are served across CalHHS programs?\",\n",
    "#              \"What is the breakdown by program?\",\n",
    "#              \"Has this changed over time?\",\n",
    "#              \"Change over time by program?\"\n",
    "#             ]\n",
    "\n",
    "question = questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_prompt = nl2sqlbq_client.table_filter_promptonly(\"Table identification initiation\")\n",
    "# print(table_prompt)\n",
    "table_chat.send_message(table_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_prompt_template = \"\"\"Using the context in the chat history, identify the table name that is most probable to contain the data requested for the question given below.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "for question in questions:\n",
    "    q_prompt = q_prompt_template.format(question=question)\n",
    "    table_identified = get_chat_response(table_chat, q_prompt)\n",
    "    print(\"Table identified for queestion :'\", question, \"' is: \", table_identified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    q_prompt = q_prompt_template.format(question=question)\n",
    "    table_identified = get_chat_response(table_chat, q_prompt)\n",
    "    print(\"Table identified for queestion :'\", question, \"' is: \", table_identified)\n",
    "\n",
    "    try:\n",
    "        previous_question_sql = sql_chat.history[-1]\n",
    "    except:\n",
    "        previous_question_sql = \"\"\n",
    "\n",
    "    sql_prompt = nl2sqlbq_client.generate_sql_few_shot_promptonly(question, table_name=table_identified, prev_sql=previous_question_sql)\n",
    "    # print(sql_prompt)\n",
    "    sql_gen = get_chat_response(sql_chat, sql_prompt)\n",
    "    print(\"Generated SQL for question : \", question, \" is \\n \", sql_gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How has participation in CalFresh changed since 2015?\"\n",
    "table_identified = get_chat_response(table_chat, q_prompt)\n",
    "previous_question_sql=\"\"\n",
    "sql_prompt = nl2sqlbq_client.generate_sql_few_shot_promptonly(question, table_name=table_identified, prev_sql=previous_question_sql)\n",
    "\n",
    "sql_chat = model.start_chat()\n",
    "\n",
    "sql_resp = sql_chat.send_message(sql_prompt, tools=[sql_tools])\n",
    "txt = get_text(sql_resp)\n",
    "if txt:\n",
    "    print(\"Response from chat\", txt)\n",
    "else:\n",
    "    fname = get_function_name(sql_resp)\n",
    "    # fname = sql_resp.candidates[0].content.parts[0].function_call.name\n",
    "    print(\"Function to call\", fname)\n",
    "    func_args = get_function_args(sql_resp)\n",
    "    # fargs = sql_resp.candidates[0].content.parts[0].function_call.args\n",
    "    print(\"function call arguments =\", func_args)\n",
    "    print(call_api(fname, func_args['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fserv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
