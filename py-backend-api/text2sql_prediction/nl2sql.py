# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the License);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https:#www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re
import os
import json
import typing
import sqlglot
import vertexai
import datetime
import langchain
import ipywidgets
import sqlalchemy
import pandas as pd
from pathlib import Path
from functools import reduce
from uuid import UUID, uuid4
from pygments import highlight
from google.cloud import storage
from pygments.lexers import SqlLexer
from itables import to_html_datatable
from langchain import agents as lc_agents
from pygments.formatters import HtmlFormatter
from IPython.core.display import display, HTML
from langchain_experimental import sql as lce_sql
from pydantic import BaseModel, field_serializer, Field
from sqlglot.optimizer.qualify_columns import qualify_columns

from functools import wraps
import time


def timeit(func):
    @wraps(func)
    def timeit_wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        total_time = end_time - start_time
        # print(f'Function {func.__name__}{args} {kwargs} Took {total_time:.4f} seconds')
        return result
    return timeit_wrapper



class BaseResult(BaseModel):
  """
  The base class to collect results from SQL generation
  """
  strategy: typing.Literal['prompt', 'queryfix', 'agent']
  # which strategy was used for SQL Generation
  raw_query: str | None
  # The generated query
  query: str | None
  # The transformed query
  error: str | None
  # The error encoundered (if any)
  summary: str | None
  # Summary generated by the chain (if any)
  intermediate_steps: list[typing.Any] | None
  # Intermediate steps taken by the agent (if any)

  @field_serializer('intermediate_steps')
  # internal Pydantic Serializer
  def serialize_intermediate_steps(self, intermediate_steps: list[typing.Any] | None, _info):
    return [
      {
        "input" : json.loads(step[0].json()),
        "output" : step[1]
      } if isinstance(step, tuple) else step
      for step in intermediate_steps
    ] if intermediate_steps else intermediate_steps

class ExecResult(BaseModel):
  """
  Collects results from SQL Execution
  """
  class Config:
    """
    Configuration class for pydantic
    """
    arbitrary_types_allowed = True
  order: int
  # The sequence number in which this was run
  result: BaseResult | None
  # The base result object
  exception: str | None
  # Exceptions encountered, if any
  exception_level: typing.Literal['inner', 'sqlgen', 'rowcount', 'data'] | None
  # Where was the exception encountered, if any
  row_count: int | None
  # how many rows were returned
  data: pd.DataFrame | None
  # the data returned from the SQL

class ResultSet(BaseModel):
  """
  The final class providing the results.
  """
  class Config:
    """
    Configuration class for pydantic
    """
    arbitrary_types_allowed = True
  id: UUID = Field(default_factory=uuid4)
  # unique identifier
  question: str
  # the user provided question
  core_params_log: dict[str, typing.Any]
  # the intantiation parameters
  early_termination: bool
  # whether early termination was used
  logs: dict[str, ExecResult]
  # a list execution results
  latest_data: pd.DataFrame | None
  # final data outcome
  latest_sql: str | None
  # final query outcome

  @timeit
  def model_post_init(self, __context) -> None:
    # Logs the object to GCS
    try:
      if self.core_params_log.get("log_bucket"):
          storage.Client().get_bucket(
            self.core_params_log.get("log_bucket")
          ).blob(
            "ws1_demoisntance/" + datetime.datetime.now().strftime(
              "%Y%m%d/"
            ) + str(self.id) + ".json"
          ).upload_from_string(
            data=json.dumps(
              {
                "question": self.question,
                "core_params_log": self.core_params_log,
                "early_termination": self.early_termination,
                "latest_sql": self.latest_sql,
                "logs" : {
                  k : json.loads(
                    v.model_dump_json(exclude={'data'})
                  ) for k, v in self.logs.items()
                }
              },
              indent = 4
            ),
            content_type='application/json'
          )
    except Exception as e:
      print(f"Could not log outputs: {e}")

def show(result:ResultSet, simple_output: bool = False):
  """
  Utility function for displaying results.
  """
  try:
    assert not simple_output
    widget_id = ipywidgets.Output()
    widget_code = ipywidgets.Output()
    widget_data = ipywidgets.Output()
    
    widget_id.layout=ipywidgets.Layout(height='10%')
    widget_code.layout=ipywidgets.Layout(height='90%')
    widget_data.layout=ipywidgets.Layout(width='40%')

    with widget_code:
      display(
        HTML(
          f"Execution ID : <B>{result.id}</B>"
        )
      )

    with widget_code:
      display(
        HTML(
          highlight(
            result.latest_sql, 
            SqlLexer(), 
            HtmlFormatter()
          )
        )
      )

    with widget_data:
      display(HTML(to_html_datatable(result.latest_data)))

    hbox = ipywidgets.HBox(
      [
        ipywidgets.VBox(
          [
            widget_id, 
            widget_code
          ], 
          layout=ipywidgets.Layout(width='60%')
        ), 
        widget_data
      ], 
      layout=ipywidgets.Layout(width='100%')
    )
    return hbox

  except Exception as e:
    if not simple_output:
      print(f"Could not visualize : {e}")

  print('-'*25, f"Run ID: {result.id}", '-'*25, sep='\n')
  print('-'*25, result.latest_sql, '-'*25, sep='\n')
  return result.latest_data

class AskBQ(BaseModel):
  """
  Core NL2SQL class
  """

  location: str
  project_id: str
  dataset_id: str
  table_names: list[str]
  enum_option_limit: int = 5
  result_row_limit: int = 1000
  log_bucket: str | None = None
  data_dict_loc: str | None = None
  postprocessors: list[typing.Literal['case_handler_transform']]
  executor_chain: list[typing.Literal['prompt_strategy', 'queryfix_strategy', 'agent_strategy']]

  @timeit
  def case_handler_transform(self, node: sqlglot.expressions.Select) -> sqlglot.expressions.Select:
    """
    implements case-handling mechanism transformation
    """
    if (
      isinstance(node, sqlglot.expressions.EQ) and
      node.find_ancestor(sqlglot.expressions.Where) and
      len(operands := list(node.unnest_operands())) == 2 and
      isinstance(literal := operands.pop(), sqlglot.expressions.Literal) and
      isinstance(predicate := operands.pop(), sqlglot.expressions.Column) and
      self._db_schema.get(predicate.table, {}).get(predicate.name, None) == 'VARCHAR'
    ):
      return sqlglot.parse_one(f"LOWER({predicate}) = '{literal.this.lower()}'")
    else:
      return node

  @timeit
  def postprocessor(self, raw_query:str) -> str:
    """
    Applies all transformations
    """
    parsed_query = qualify_columns(
      sqlglot.parse_one(raw_query), 
      self._db_schema,
      expand_alias_refs=False
    )
    
    for postprocess_func in self.postprocessors:
        parsed_query = parsed_query.transform(getattr(self, postprocess_func))
    return parsed_query.sql(pretty=True)
    
  @timeit
  def model_post_init(self,  __context, col_values_distribution=False,) -> None:
    
    """
    Generates the required attributes
    """    
    self._core_params_log = {
      "location": self.location,
      "project_id" : self.project_id,
      "dataset_id" : self.dataset_id,
      "table_names" : self.table_names,
      "enum_option_limit" : self.enum_option_limit,
      "result_row_limit" : self.result_row_limit,
      "log_bucket" : self.log_bucket,
      "data_dict_loc" : self.data_dict_loc,
      "postprocessors" : self.postprocessors,
      "module_path" : str(os.path.abspath(__file__))
    }

    vertexai.init(
      project=self.project_id, 
      location=self.location
    )
    
    self._engine = sqlalchemy.engine.create_engine(
      f"bigquery://{self.project_id}/{self.dataset_id}"
    )
    
    custom_table_info = None
    if self.data_dict_loc:
      data_dict = json.loads(Path(self.data_dict_loc).resolve().read_text())
      temp_db = langchain.utilities.SQLDatabase(
        engine=self._engine,
        metadata=sqlalchemy.MetaData(bind=self._engine),
        include_tables=self.table_names,
        custom_table_info = custom_table_info,
        view_support=True
      )
      custom_table_info = {
        re.search('CREATE TABLE `(.*)`', td).group(1) : td
        for td in temp_db.table_info.split('\n\n\n')
      }
      for tname in custom_table_info:
        tdesc = data_dict[tname]
        cdescs = []
        for col in temp_db._metadata.tables[tname]._columns:
          colname_dict = str(f"{tname}.{col.name}")
          if colname_dict in data_dict.keys():
            cdesc = str(col.name) + " : " + str(data_dict[colname_dict])
          # if col_values_distribution and str(col.type) == "VARCHAR":
          #   if pd.read_sql(
          #     sql=f"SELECT COUNT(DISTINCT {col.name}) <= {self.enum_option_limit} FROM {tname}",
          #     con=self._engine
          #   ).values[0][0]:
          #     cdesc += " This column contains only these values : \"" + (
          #       "\", \"".join(
          #         filter(
          #           lambda x: x is not None,
          #           pd.read_sql(
          #             sql=f"SELECT DISTINCT {col.name} AS vals FROM {tname}",
          #             con=self._engine
          #           )["vals"].to_list()
          #         )
          #       )
          #     ) + "\"."
          cdescs.append(cdesc)
        custom_table_info[tname] = (
          "\n\n"
          "Table Name : {tname}\n"
          "Description : {tdesc}\n"
          "This table has the following columns : \n"
          "{cdescs}\n"
          "Here is the create statement for this table:\n"
          "{table_info}"
          "\n\n"
        ).format(
          tname=tname,
          tdesc=tdesc,
          cdescs='\n'.join(cdescs),
          table_info=custom_table_info[tname].strip()
        )
        

    self._db = langchain.utilities.SQLDatabase(
      engine=self._engine,
      metadata=sqlalchemy.MetaData(bind=self._engine),
      include_tables=self.table_names,
      custom_table_info = custom_table_info,
      view_support=True
    )
    
    self._db_schema = {
      t: {
        c.name: str(c.type) for c in self._db._metadata.tables[t].columns
      } for t in self._db._usable_tables
    }

    self._llm = langchain.llms.VertexAI(temperature=0, model_name="text-bison-32k", max_output_tokens=2048)

    self._prompt_strat = langchain.chains.create_sql_query_chain(
      self._llm,
      self._db,
      k=self.result_row_limit
    )

    self._queryfix_strat = lce_sql.SQLDatabaseSequentialChain.from_llm(
      self._llm,
      self._db,
      verbose=False,
      top_k=self.result_row_limit,
      return_intermediate_steps = True,
      use_query_checker = True
    )

    self._agent_strat = lc_agents.create_sql_agent(
      llm=self._llm,
      top_k=self.result_row_limit,
      toolkit=lc_agents.agent_toolkits.SQLDatabaseToolkit(db=self._db, llm=self._llm),
      verbose=False,
      agent_type=lc_agents.agent_types.AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    )
    self._agent_strat.return_intermediate_steps=True
    self._agent_strat.handle_parsing_errors=True
    # print(custom_table_info)

  @timeit
  def _prompt_strat_executor(self, question:str) -> BaseResult :
    """
    Executor for simple prompting based Langchain Chain
    """
    raw_query = None
    query = None
    error = None
    
    try:
      raw_query = self._prompt_strat.invoke(
        {"question":question}
      ).replace(';', '').replace('sql```', '').replace('```', '')
    except Exception as e:
      error = str(e)
    else:
      try:
        query = self.postprocessor(raw_query)
      except:
        query = raw_query    

    return BaseResult(
      strategy = 'prompt',
      raw_query = raw_query,
      query = query,
      error = error,
      summary = None,
      intermediate_steps = None
    )

  @timeit
  def _queryfix_strat_executor(self, question:str) -> BaseResult :
    """
    Executor for TableFiltering + QueryChecker langchain chain
    """

    raw_query = None
    query = None
    error = None
    summary = None
    intermediate_steps = None

    try:
      try:
        result = self._queryfix_strat(question)
      except sqlalchemy.exc.DatabaseError as e:
        result = {'intermediate_steps' : e.intermediate_steps}
    except Exception as e:
      error = str(e)
    else:
      try:
        raw_query = next(
          (
            i['sql_cmd'].replace(';', '').replace('sql```', '').replace('```', '')
            for i in result['intermediate_steps']
            if isinstance(i, dict) and 'sql_cmd' in i.keys()
          ),
          None
        )
      except Exception as e:
        error = str(e)
      else:
        summary = result.get('result')
        intermediate_steps = result.get('intermediate_steps')
        try:
          query = self.postprocessor(raw_query)
        except:
          query = raw_query    

    finally:
      return BaseResult(
        strategy = 'queryfix',
        query = query,
        raw_query = raw_query,
        error = error,
        summary = summary,
        intermediate_steps = intermediate_steps
      )

  @timeit
  def _agent_strat_executor(self, question:str) -> BaseResult :
    """
    Executor for Agent based Langchain chain
    """

    raw_query = None
    query = None
    error = None
    summary = None
    intermediate_steps = None

    try:
      result = self._agent_strat(question)
    except Exception as e:
      error = str(e)
    else:
      try:
        raw_query = next(
          map(
            lambda x: x[0].tool_input.replace(';', '').replace('sql```', '').replace('```', ''),
            filter(
              lambda x: x[0].tool in ['sql_db_query', 'sql_db_query_checker'],
              reversed(result.get('intermediate_steps', []))
            )
          ),
          None
        )
      except Exception as e:
        error = str(e)
      else:
        summary = result.get('output')
        intermediate_steps = result.get('intermediate_steps')
        try:
          query = self.postprocessor(raw_query)
        except:
          query = raw_query    

    finally:
      return BaseResult(
        strategy = 'agent',
        query = query,
        raw_query = raw_query,
        error = error,
        summary = summary,
        intermediate_steps = intermediate_steps
      )

  @timeit
  def execute(self, query):
    """
    Executes given query and returns dataframe
    """
    with self._engine.connect() as conn:
      df = pd.read_sql(
        sql=query,
        con=conn.connection
    )
    return df

  @timeit
  def __call__(self, question:str, early_termination:bool = True) -> ResultSet:
    """
    Main NL2SQL trigger function. Safely loops through all executors and returns result.
    """
    strategy_dict = {'prompt_strategy':self._prompt_strat_executor,
                     'queryfix_strategy':self._queryfix_strat_executor,
                     'agent_strategy':self._agent_strat_executor}
    exec_order = [strategy_dict[v] for v in self.executor_chain]
    # exec_order = [
    #   self._prompt_strat_executor,
    #   self._queryfix_strat_executor,
    #   self._agent_strat_executor
    # ]

    exec_log = {}
    latest_sql = None
    latest_data = None
    
    for idx, executor in enumerate(exec_order):
      result = None
      exception = None
      exception_level = None
      row_count = None
      data = None
      try:
        result = executor(question)
        # print(executor, result.query)
      except Exception as e:
        exception = str(e)
        exception_level = 'sqlgen'
      else:
        if result.error:
          exception = result.error
          exception_level = 'inner'
        elif not result.query:
          exception = f"Incorrect query Generated: {result.error}"
          exception_level = 'inner'
        else:
          try:
            row_count = self.execute(
              f"SELECT COUNT(*) AS rowcount FROM ({result.query});"
            ).to_dict('split')['data'].pop().pop()
          except Exception as e:
            exception = str(e)
            exception_level = 'rowcount'
          else:
            try:
              data = self.execute(
                f"SELECT * FROM ({result.query}) LIMIT {self.result_row_limit};"
              )
            except Exception as e:
              exception = str(e)
              exception_level = 'data'
      finally:
        exec_result = ExecResult(
          order=idx,
          result=result,
          exception=exception,
          exception_level=exception_level,
          row_count=row_count,
          data=data
        )

        exec_log[executor.__name__] = exec_result

        if (
          (data is not None) and
          (result is not None) and
          (result.query is not None)
        ):
          latest_sql = result.query
          latest_data = data

        if early_termination and (not exception):
          return ResultSet(
            question = question,
            early_termination = early_termination,
            core_params_log = self._core_params_log,
            logs=exec_log,
            latest_sql=latest_sql,
            latest_data=latest_data
          )

    return ResultSet(
      question = question,
      early_termination = early_termination,
      core_params_log = self._core_params_log,
      logs=exec_log,
      latest_sql=latest_sql,
      latest_data=latest_data
    )

 
# Function definition for table_filter
@timeit
def table_filter(question):

    llm = langchain.llms.VertexAI(temperature=0, model_name="text-bison@latest", max_output_tokens=2048)

    prompt = f'''You are a database expert at selecting a table from a list of tables based on their description.
    For the provided question choose what is the table_name most likely to be relevant.
    Only select a table from the following list and their description-keywords. Select multiple tables if applicable.

    Table name | description-keywords
    authorizations_search | This table contains information about all the transactions in sales and their details like status, date, network, payment methods etc. Auth, Sale, Flow, Transaction
    settlement_search | The Approved Transactions are now sent for Settlement. Settlement transactions are the first step in Funding the merchant. Keywords are Settled, Settlement, Settle
    chargebacks_search | Dispute, Chargeback
    funding_search | Fund, Deposit
    
    Question: {question}
    '''
    result = llm(prompt)

    segments = result.split(',')
    tables_list = []
    
    for segment in segments:
        segment = segment.strip()
        if ':' in segment:
            value = segment.split(':')[-1].strip()
            tables_list.append(value)
        else:
            tables_list.append(segment)
            
    return tables_list
